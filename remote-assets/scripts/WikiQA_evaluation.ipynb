{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiQA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n",
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "from fairscale.nn.model_parallel import initialize_model_parallel\n",
    "import torch.distributed as dist\n",
    "\n",
    "master_port=\"12380\"\n",
    "# Set up distributed environment\n",
    "def setup(rank, world_size):\n",
    "    \"Sets up the process group and configuration for PyTorch Distributed Data Parallelism\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    # os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    # Prevent conflict with jupyter in case.\n",
    "    os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "    # Initialize the process group\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "def cleanup():\n",
    "    \"Cleans up the distributed environment\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "setup(0, 1)\n",
    "initialize_model_parallel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from logging import getLogger\n",
    "from typing import List\n",
    "logger = getLogger()\n",
    "\n",
    "tokenizer = LlamaTokenizer(\"./tokenizers/tokenizer.model\")\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token = False\n",
    "\n",
    "import random\n",
    "class PromptDataset(Dataset):\n",
    "\n",
    "    def __init__(self, prompt_dataset, answer_dataset) -> None:\n",
    "        super().__init__()\n",
    "        self.prompt_dataset = prompt_dataset\n",
    "        self.answer_dataset = answer_dataset\n",
    "\n",
    "        c = list(zip(self.prompt_dataset, self.answer_dataset))\n",
    "\n",
    "        random.shuffle(c)\n",
    "\n",
    "        self.prompt_dataset, self.answer_dataset = zip(*c)\n",
    "\n",
    "        assert len(self.prompt_dataset) == len(self.answer_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"prompt\": self.prompt_dataset[idx],\n",
    "            \"answer\": self.answer_dataset[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, data, max_seq_len):\n",
    "        self.input = data\n",
    "        self.prompt_dataset = []\n",
    "        self.answer_dataset = []\n",
    "        for val in self.input:\n",
    "            question = val[\"question\"]\n",
    "            answer = val[\"answer\"]\n",
    "            if len(question.split(\" \")) < max_seq_len:\n",
    "                self.prompt_dataset.append(question)\n",
    "                self.answer_dataset.append(answer)\n",
    "            assert len(self.prompt_dataset) == len(self.answer_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"prompt\": self.prompt_dataset[idx],\n",
    "            \"answer\": self.answer_dataset[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/team-14-azure-user/Desktop/team14/llms-sys-ml/draft-files-v1/sysml-venv/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from eval.evaluations import eval_general\n",
    "from typing import List, Optional\n",
    "\n",
    "from generators.generator_with_runtime_copy import Llama, Dialog\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runnn():\n",
    "    #ckpt_dir = \"./checkpoints/baseline_slimpj_full_try2.pt\"\n",
    "    ckpt_dir = \"./checkpoints/novel_proto_8_layer.pt\"\n",
    "    tokenizer_path = \"./tokenizers/tokenizer.model\"\n",
    "    temperature: float = 0.6\n",
    "    top_p: float = 0.9\n",
    "    max_seq_len: int = 512\n",
    "    batch_size: int = 32\n",
    "    max_gen_len: Optional[int] = None\n",
    "\n",
    "    def prediction(\n",
    "        model,\n",
    "        infer_dataset,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_seq_len: int = 512,\n",
    "        batch_size: int = 32,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        idx=None,\n",
    "    ):\n",
    "        predicted_sequences = []\n",
    "        ground_truths = []\n",
    "        for i, step in enumerate(range(0, batch_size, batch_size)):\n",
    "            if step % 10:\n",
    "                print(f\"Batch: {i}, Samples: {step}\")\n",
    "            batch = infer_dataset[step : step + batch_size]\n",
    "\n",
    "            prompts: List[str] = batch[\"prompt\"]\n",
    "            answers = batch[\"answer\"]\n",
    "            ground_truths += answers\n",
    "\n",
    "            predicted_sequence = model.text_completion(\n",
    "                prompts,  # type: ignore\n",
    "                max_gen_len=max_gen_len,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            predicted_sequences += predicted_sequence\n",
    "        return predicted_sequences, ground_truths\n",
    "\n",
    "    def save_inference_results(\n",
    "        evaluation_result: dict,\n",
    "        sources_sequences: list,\n",
    "        predicted_sequences: list,\n",
    "        ground_truths: list,\n",
    "    ):\n",
    "        # save as a json file\n",
    "        df = {\n",
    "            \"eval\": evaluation_result,\n",
    "            \"prompts\": sources_sequences,\n",
    "            \"results\": predicted_sequences,\n",
    "            \"labels\": ground_truths,\n",
    "        }\n",
    "        with open(\"testQA.json\", \"w+\", encoding=\"utf-8\") as file:\n",
    "            json.dump(df, file, ensure_ascii=False)\n",
    "\n",
    "    model = Llama.build(\n",
    "        ckpt_dir=ckpt_dir,\n",
    "        tokenizer_path=tokenizer_path,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    from eval.data.data_utils import create_prompt_dataset\n",
    "\n",
    "    data = load_dataset(\"wiki_qa\", split=\"test\")\n",
    "    data_split = data.select(range(0, len(data)))\n",
    "    infer_dataset = EvaluationDataset(data_split, 512)\n",
    "\n",
    "    # Inference !\n",
    "    predicted_sequences, ground_truths = prediction(model, infer_dataset)\n",
    "    return predicted_sequences, ground_truths\n",
    "    # Get Accuracy/ROUGE/BLEU/...\n",
    "    # The evaluation result is stored in a dictionary. e.g. {\"accuracy\": .., \"rouge-L\": ..}\n",
    "    evaluation_result = eval_general.eval(predicted_sequences, ground_truths)\n",
    "    # if args.global_rank <= 0:  # only one process is running\n",
    "    print(\"***** Saving inference results *****\")\n",
    "    save_inference_results(\n",
    "        evaluation_result,\n",
    "        infer_dataset[:][\"prompt\"],\n",
    "        predicted_sequences,\n",
    "        ground_truths,\n",
    "    )\n",
    "\n",
    "    return evaluation_result, model.runtimes\n",
    "\n",
    "\n",
    "# result, runtime = runnn()\n",
    "\n",
    "# mean = np.mean(runtime)\n",
    "\n",
    "# low = np.percentile(runtime, 5)\n",
    "# median = np.percentile(runtime, 50)\n",
    "# high = np.percentile(runtime, 95)\n",
    "# std = np.std(runtime)\n",
    "# runtime_metrics = [mean, median, low, high, std]\n",
    "\n",
    "# metrics = [\"mean\", \"median\", \"low\", \"high\", \"std\"]\n",
    "# tables = {metric: {} for metric in metrics}\n",
    "# print(tables)\n",
    "# for key, val in result.items():\n",
    "#     for attr, value in val.items():\n",
    "#         tables[attr][key] = value\n",
    "\n",
    "# count = 0\n",
    "# for metric, values in tables.items():\n",
    "#     print(f\"Table for {metric}:\")\n",
    "#     res = \"\"\n",
    "#     for v in values:\n",
    "#         res += str(values[v]) + \" \"\n",
    "#     res += str(runtime_metrics[count])\n",
    "#     count += 1\n",
    "#     print(res)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5296310.842741208, 5341310.0, 4689967.3, 5562894.0, 3369310.761344379)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(runtime)\n",
    "\n",
    "low = np.percentile(runtime, 5)\n",
    "median = np.percentile(runtime, 50)\n",
    "high = np.percentile(runtime, 95)\n",
    "std = np.std(runtime)\n",
    "runtime_metrics = [mean, median, low, high, std]\n",
    "mean, median, low, high, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': {}, 'median': {}, 'low': {}, 'high': {}, 'std': {}}\n",
      "Table for mean:\n",
      "0.009170254200980296 0.0014920185621561208 0.08956978928953643 0.08650932804574835 3.258996212121221 5296310.842741208\n",
      "\n",
      "Table for median:\n",
      "0.009680374259169351 0.00157209915425711 0.09458063018927546 0.091370973407304 3.4393939393939394 5341310.0\n",
      "\n",
      "Table for low:\n",
      "0.009144539330610981 0.0013564956303575384 0.0913219874170982 0.08936381649239213 3.3267045454545454 4689967.3\n",
      "\n",
      "Table for high:\n",
      "0.010249341575857373 0.0018227574042409792 0.09797446146321216 0.09331478087729288 3.5615530303030303 5562894.0\n",
      "\n",
      "Table for std:\n",
      "0.01118627798610428 0.004564195435031823 0.06736035281845515 0.039505201758894055 2.3658068338622282 3369310.761344379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = ['mean', 'median', 'low', 'high', 'std']\n",
    "tables = {metric: {} for metric in metrics}\n",
    "print(tables)\n",
    "for key, val in result.items():\n",
    "    for attr, value in val.items():\n",
    "        tables[attr][key] = value\n",
    "\n",
    "count = 0\n",
    "for metric, values in tables.items():\n",
    "    print(f\"Table for {metric}:\")\n",
    "    res = \"\"\n",
    "    for v in values:\n",
    "        res += str(values[v]) + ' '\n",
    "    res += str(runtime_metrics[count])\n",
    "    count += 1\n",
    "    print(res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count = 0\n",
    "for metric, values in tables.items():\n",
    "    print(f\"Table for {metric}:\")\n",
    "    res = []\n",
    "    for v in values:\n",
    "        res.append(values[v])\n",
    "    res.append(runtime_metrics[count])\n",
    "    count += 1\n",
    "    df = pd.DataFrame([res])\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 1.44 seconds\n"
     ]
    }
   ],
   "source": [
    "predict, gt = runnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sysml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
